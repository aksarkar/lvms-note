\documentclass[reqno,11pt]{amsart}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage[isbn=false,eprint=false,url=false,doi=false,date=year,style=nature]{biblatex}
\addbibresource{/home/aksarkar/research/mit/reading/reading.bib}

\title{Latent variable models}
\author{Abhishek Sarkar}
\date{}

\DeclareMathOperator\diag{diag}
\DeclareMathOperator\E{E}
\DeclareMathOperator\KL{\mathcal{KL}}
\DeclareMathOperator\N{\mathcal{N}}
\newcommand\mb{\mathbf{B}}
\newcommand\me{\mathbf{E}}
\newcommand\mi{\mathbf{I}}
\newcommand\mm{\mathbf{M}}
\newcommand\mr{\mathbf{R}}
\newcommand\msigma{\boldsymbol{\Sigma}}
\newcommand\ms{\mathbf{S}}
\newcommand\mw{\mathbf{W}}
\newcommand\mx{\mathbf{X}}
\newcommand\my{\mathbf{Y}}
\newcommand\vb{\mathbf{b}}
\newcommand\vmu{\boldsymbol{\mu}}
\newcommand\vm{\mathbf{m}}
\newcommand\vphi{\boldsymbol{\phi}}
\newcommand\vs{\mathbf{s}}
\newcommand\vtheta{\boldsymbol{\theta}}
\newcommand\vx{\mathbf{x}}
\newcommand\vz{\mathbf{z}}

\begin{document}
\maketitle

\section{Introduction}

The essential aspect of a \emph{latent variable model} (LVM) is that each
observation $\vx_i$ (which is $p$-dimensional, say), has an associated latent
variable $\vz_i$ (which is $k$-dimensional). Usually, one assumes $k$ is much
less than $p$, corresponding to an assumption that the differences in the
high-dimensional observations $\vx_i$ are (mostly) explained by differences in
the lower-dimensional latent variables $\vz_i$.

\section{A simple example: Probabilistic PCA}

One of the simplest examples of an LVM is \emph{probabilistic PCA} (PPCA)
\cite{10.1111/1467-9868.00196}
%
\begin{align}
  \vx_i \mid \vz_i, \mw, \sigma^2 &\sim \N(\mw \vz_i, \sigma^2 \mi)\\
  \vz_i &\sim \N(\boldsymbol{0}, \mi).
\end{align}
%
In this model, observations $\vx_i$ are generated by first applying the linear
transform $\mw$ to the low-dimensional latent variable $\vz_i$, and then adding
multivariate Gaussian noise with zero mean and covariance $\sigma^2 \mi$. There
are two inference goals for PPCA:

\begin{enumerate}
\item Estimate $\mw, \sigma^2$ given observed data $\vx_1, \ldots, \vx_n$. This
  involves maximizing the marginal likelihood, that is, solving the
  optimization problem
  %
  \begin{align}
    \hat\mw, \hat\sigma^2 &= \arg\max_{\mw, \sigma^2} \sum_{i=1}^n \ln p(\vx_i \mid \mw, \sigma^2)\\
    &= \arg\max_{\mw, \sigma^2} \sum_{i=1}^n \ln \left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \mw, \sigma^2) p(\vz_i)\,d\vz_i\right),
    \label{eq:eb}
  \end{align}
  %
  where $\mathcal{Z}$ denotes the set of values $\vz_i$ can take.
  
\item Estimate $\vz_i$ given $\vx_i$. This involves computing the posterior
  mean $\E[\vz_i \mid \vx_i, \hat\mw, \hat\sigma^2]$, using the result
  $\hat\mw, \hat\sigma^2$ from the previous step.
\end{enumerate}

How does one estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$? In
the case of PPCA, the integrals in equation \eqref{eq:eb} have closed forms
(following from properties of the Gaussian distribution); thus,
%
\begin{equation}
  \hat\mw, \hat\sigma^2 = \arg\max_{\mw, \sigma^2} \sum_{i=1}^n -\frac{1}{2}\ln\det(\mw\mw' + \sigma^2 \mi) - \frac{1}{2} \vx_i' (\mw\mw' + \sigma^2 \mi)^{-1} \vx_i.
  \label{eq:ppca}
\end{equation}
%
One can further show that the solution to the optimization problem
\eqref{eq:ppca} is unique and has closed forms related to the
eigendecomposition of $\mx'\mx$, where $\mx$ denotes the $n \times p$ matrix
$(\vx_1, \ldots, \vx_n)'$. Specifically, $\hat\mw$ is the matrix of the first
$k$ eigenvectors (having largest eigenvalues), and $\hat\sigma^2$ is the ratio
of the sum of the remaining $p - k$ eigenvalues to the sum of all of the
eigenvalues. (Equivalently, the closed forms are related to the singular value
decomposition of $\mx$.)

How does one estimate $\vz_i$ given $\vx_i$? One can think of $p(\vx_i \mid
\vz_i, \mw, \sigma^2)$ as the likelihood and $p(\vz_i)$ as the prior in a
Bayesian setting. Both are Gaussian; therefore, the posterior is also Gaussian
(due to conjugacy):
%
\begin{equation}
  \vz_i \mid \vx_i, \mw, \sigma^2 \sim \N((\mw'\mw + \sigma^2 \mi)^{-1} \mw' \vx_i, \sigma^2 (\mw'\mw + \sigma^2 \mi)^{-1}),
\end{equation}
%
yielding the closed-form posterior mean $(\hat\mw'\hat\mw + \hat\sigma^2
\mi)^{-1} \hat\mw' \vx_i$.

\section{The general scheme}

An LVM is typically given as a joint distribution
%
\begin{equation}
  p(\vx_i, \vz_i \mid \vtheta) = p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta),
  \label{eq:joint}
\end{equation}
%
where $\vtheta$ denotes a collection of additional model parameters. One can
think of $p(\vx_i \mid \vz_i, \vtheta)$ as the likelihood, and $p(\vz_i \mid
\vtheta)$ as the prior in a Bayesian setting. There are two inference goals for
LVMs:

\begin{enumerate}
\item Estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$. This
  involves maximizing the marginal likelihood
  %
  \begin{equation}
    \hat\vtheta = \arg\max_{\vtheta} \sum_{i=1}^n \ln \left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta)p(\vz_i \mid \vtheta) \,d\vz_i\right),
  \end{equation}
  %
  where $\mathcal{Z}$ denotes the set of values $\vz_i$ can take. This problem
  may not be tractable, and may require approximate inference methods. (In the
  case of PPCA, the integrals had closed forms.) In a Bayesian setting, this
  task corresponds to estimating the prior from the data, which is termed
  \emph{empirical Bayes}. (In the case of PPCA, the prior did not have any free
  parameters to be estimated; however, in other models, the prior does have
  such parameters.)
  
\item Estimate $\vz_i$ given $\vx_i, \hat\vtheta$. This involves computing the
  posterior mean, by combining the likelihood $p(\vx_i \mid \vz_i,
  \hat\vtheta)$ with the prior $p(\vz_i \mid \hat\vtheta)$ estimated in the
  previous step. As above, this may not be tractable, and may require
  approximate inference methods. (In the case of PPCA, the prior was conjugate
  to the likelihood; therefore, the posterior mean had a closed form.)
\end{enumerate}
%
One may also want to generate new observations $\vx$. However, this is not
``inference'' as commonly understood in statistics; it is merely a case of
random sampling from the distribution $p(\vx \mid \hat\vtheta)$, which is
achieved by first sampling $\vz \sim p(\vz \mid \hat\vtheta)$, then sampling $\vx
\sim p(\vx \mid \vz, \hat\vtheta)$.

\section{Variational autoencoder}

A more complex LVM is the \emph{variational autoencoder} (VAE)
\cite{DBLP:journals/corr/KingmaW13}
%
\begin{align}
  \vx_i \mid \vz_i, \vtheta &\sim \N(f(\vz_i), \sigma^2 \mi)\\
  \vz_i &\sim \N(\boldsymbol{0}, \mi),
\end{align}
%
where $f$ denotes a neural network with $k$-dimensional input and
$p$-dimensional output, and $\vtheta$ denotes $\sigma^2$ and the parameters of
$f$. As a simple example, suppose $f$ is a fully-connected feed-forward network
with one hidden layer and a linear output layer, which can be written
%
\begin{equation}
  f(\vz_i) = \mw_1 h(\mw_0 \vz_i + \vb_0) + \vb_1,
\end{equation}
%
where $h$ is a non-linearity (say, ReLU) applied element-wise. In this case,
$\vtheta = (\sigma^2, \mw_0, \mw_1, \vb_0, \vb_1)$, and the VAE is a non-linear
version of PPCA. Specifically, one generates observations $\vx_i$ by applying
the non-linear transform $f$ to the low-dimensional latent variable $\vz_i$,
and then adding multivariate Gaussian noise with zero mean and covariance
$\sigma^2 \mi$. As was the case for PPCA, the transform $f$ will be learned
from the data (in PPCA, the transform was $\mw$); however, unlike PPCA, in
general it does not have a closed form.

How does one estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$? The
integrals in equation \eqref{eq:eb} do not have closed forms for the VAE, so one needs
to take a different strategy. The strategy taken for VAEs is \emph{variational
inference} \cite{10.1080/01621459.2017.1285773}.

\subsection{Variational inference}

Recall the objective function,
%
\begin{align}
  &\quad \max_{\vtheta} \sum_{i=1}^n \ln\left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta) \,d\vz_i\right)\\
  \intertext{Introducing a distribution $q(\vz_i \mid \vphi)$ (sometimes termed
    the \emph{variational surrogate} or \emph{variational approximation}) by
    multiplying and dividing by its density, and optimizing over its parameters
    $\vphi$ also,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln\left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta) \frac{q(\vz_i \mid \vphi)}{q(\vz_i \mid \vphi)} p(\vz_i \mid \vtheta)\,d\vz_i\right) \label{eq:q}\\
  \intertext{By the definition of expectation with respect to $q(\vz_i \mid \vphi)$,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln\left(\E_q\left[\frac{p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta)}{q(\vz_i \mid \vphi)}\right]\right)\\
  \intertext{Finally, one can exchange log and expectation using Jensen's
    inequality, yielding the \emph{evidence lower bound} (ELBO)}
  &\geq \max_{\vphi, \vtheta} \sum_i \E_q[\ln p(\vx_i \mid \vz_i, \vtheta) + \ln p(\vz_i \mid \vtheta) - \ln q(\vz_i \mid \vphi)] \label{eq:elbo}.
\end{align}
%
The ELBO \eqref{eq:elbo} is the objective function for variational inference in
LVMs, which can be optimized by a number of different algorithms, for example,
gradient descent. (Variational inference in other models involves an objective
function with similar form, involving the expectation of the log joint
probability density $\E_q[\ln p]$ and the expectation of the log variational
density $\E_q[\ln q]$.)

Before detailing how the ELBO \eqref{eq:elbo} is optimized for VAEs
specifically, first note that equation \eqref{eq:q} holds for any distribution
$q$ (since we multiplied and divided by its density); however, one can show
that the optimal $q$ (equivalently, optimal $\vphi$) is the true posterior
$p(\vz_i \mid \vx_i, \vtheta)$ (equivalently, the parameters of this
distribution) \cite{Neal1998}. Adding and subtracting $\ln p(\vx_i \mid
\vtheta)$ from the ELBO \eqref{eq:elbo},
%
\begin{align}
  \begin{split}
    &= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta) + \ln p(\vx_i \mid \vz_i, \vtheta) + \ln p(\vz_i \mid \vtheta)\\
      &\qquad\qquad\qquad - \ln p(\vx_i \mid \vtheta) - \ln q(\vz_i \mid \vphi)]\\
  \end{split}
  \intertext{Using the definition of conditional probability,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta) - \ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)]\\
  \intertext{Using linearity of expectation,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta)] - \E_q[\ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)]
  \intertext{Since $p(\vx_i \mid \vtheta)$ does not depend on $\vz_i$,
    $\E_q[\ln p(\vx_i \mid \vtheta)] = \ln p(\vx_i \mid \vtheta)$, and}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln p(\vx_i \mid \vtheta) - \E_q[\ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)]\\
  \intertext{The latter expectation is the definition of the KL divergence
    between $q(\vz_i \mid \vphi)$ and $p(\vz_i \mid \vx_i, \vtheta)$; it equals
    zero when the two distributions are the same.}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln p(\vx_i \mid \vtheta) - \KL\left(p(\vz_i \mid \vx_i, \vtheta) \Vert\, q(\vz_i \mid \vphi)\right) \label{eq:kl}
  \intertext{Thus, the optimal $q$ (equivalently, optimal $\phi$), holding
    $\vtheta$ fixed, is the true posterior given $\vtheta$ (equivalently, its
    parameters), in which case the inequality \eqref{eq:elbo} becomes an
    equality,}
  &= \max_{\vtheta} \sum_{i=1}^n \ln p(\vx_i \mid \vtheta).
\end{align}
%
Further, from equation \eqref{eq:kl}, maximizing the ELBO with respect to
$\vphi$ holding $\vtheta$ fixed is equivalent to minimizing the KL divergence
between $q(\vz_i \mid \vphi)$ and $p(\vz_i \mid \vx_i, \vtheta)$ holding
$\vtheta$ fixed. Thus, the result will be the ``best'' approximating
distribution $q$, in the sense that it has minimum KL divergence from the true
posterior (given $\vtheta$), even when one cannot compute the true posterior!

\subsection{Solving the inference problems for VAEs}

For VAEs, one estimates $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$ by
variational inference: optimizing the ELBO \eqref{eq:elbo} with respect to
$\vphi, \vtheta$, where $\vphi$ denotes the parameters of a variational
approximation $q$. (This procedure also yields estimates $\hat\vphi$ of the
variational parameters.) Typically, assumes that the approximation $q$ is a
Gaussian distribution with unknown mean and unknown diagonal covariance matrix
%
\begin{equation}
  q(\vz_i \mid \vx_i, \vphi) = \N(\vm_i, \diag(\vs_i^2)),
  \label{eq:unamortized}
\end{equation}
%
where $\vphi = (\mm, \ms)$, $\mm$ denotes the $n \times k$ matrix $(\vm_1,
\ldots, \vm_n)'$, and $\ms$ denotes the $n \times k$ matrix $(\vs_1, \ldots,
\vs_n)'$. One makes this assumption in order that the approximate posterior
mean (and certain other quantities) will be easy to compute; the assumed
distribution is almost surely not the true posterior. In this case, the result
will be the Gaussian distribution with mean and diagonal covariance matrix
closest to the true posterior (meaning, having minimum KL divergence).

Note that the variational approximation \eqref{eq:unamortized} is a
simplification and not the way VAEs were originally described -- there is
clearly no ``auto-encoding'' going on. Nevertheless, this approach is
equivalent in the essential aspects, as we detail below (and possibly better
with regards to the objective function achieved in training). Also note that
there is considerable subsequent work on relaxing the assumption
\eqref{eq:unamortized}, in order to get more flexible variational
approximations that can be closer to the true posterior.

For VAEs, optimizing the ELBO \eqref{eq:elbo} with respect to $\vphi, \vtheta$
is still difficult since it does not have a closed form. Specifically,
$\E_q[\ln p(\vx_i \mid \vz_i, \vtheta)]$ does not have a closed form due to the
non-linear transform $f$; the remaining terms do have a closed form. (Note that
the ELBO does have a closed form for other models.) Instead, one uses
\emph{stochastic optimization}: the ELBO \eqref{eq:elbo} is replaced by a
stochastic objective function that does have a closed form, whose expected
value is the true objective function value, whose gradient can be computed
using backpropagation, and whose expected gradient is the true gradient
\cite{DBLP:journals/corr/KingmaW13}. We illustrate fitting VAEs using this
approach in an online
example\footnote{\url{https://aksarkar.github.io/singlecell-ideas/vae-unamortized.html}}.

How does one estimate $\vz_i$ given $\vx_i$? The approximate posterior mean is
simply $\vm_i$.

\subsection{Original description}

Note that VAEs are parameterized by a neural network $f$, originally termed the
\emph{decoder network} (since it ``decodes'' the low-dimensional latent
variable $\vz_i$ to produce the noiseless, uncorrupted version of the observed
$\vx_i$). As originally proposed, one chooses $q$ to also be parameterized by a
neural network, termed the \emph{encoder network} (since it ``encodes'' $\vx_i$
to produce $\vz_i$). Specifically,
%
\begin{equation}
  q(\vz_i \mid \vx_i, \vphi) = \N(m(\vx_i), \diag(s^2(\vx_i))),
  \label{eq:amortized}
\end{equation}
%
where $m, s^2$ are each $k$-dimensional outputs (denoting the mean and diagonal
of the covariance matrix, respectively) of a neural network taking
$p$-dimensional input. This choice is the origin of the name ``variational
autoencoder''.

If one assumes the variational approximation \eqref{eq:amortized}, then $\vphi$
denotes the neural network parameters of $m, s^2$. In this case, rather than
finding (and storing) an optimal $\vm_i^*, \vs_i^*$ for each $\vx_i$, one
instead learns functions $m, s^2$ that map each $\vx_i$ to its optimal
$\vm_i^*, \vs_i^*$, an approach termed \emph{amortized inference} (meaning,
amortized over observations $\vx_i$) \cite{Gershman2014}. One does not strictly
need to do this; in fact, it results in worse objective function values at
convergence if the encoder network is not powerful enough to represent the
functions mapping $\vx_i \mapsto \vm_i^*, \vs_i^*$, as we demonstrate in our
online example. The main advantages of using amortized inference are:

\begin{enumerate}
\item When $n$ is large, the number of neural network parameters of $m, s^2$
  may be smaller than the $2n \times k$ entries of $\mm, \ms$,
\item The implementation of minibatch gradient descent to optimize the ELBO
  \eqref{eq:elbo} when using an encoder network is simpler in commonly used
  libraries (e.g., \texttt{pytorch}, \texttt{tensorflow})
\item One can re-use the learned $m, s^2$ for new observations $\vx_{i'}$. If
  one assumed the simpler variational approximation \eqref{eq:unamortized},
  then to fit a new observation $\vx_{i'}$ one must re-optimize \eqref{eq:elbo}
  with respect to $\vm_{i'}, \vs_{i'}$ holding $\vtheta$ fixed.
\end{enumerate}

\printbibliography

\end{document}
