\documentclass[reqno,11pt]{amsart}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage[isbn=false,eprint=false,url=false,doi=false,date=year,style=nature]{biblatex}
\addbibresource{/home/aksarkar/research/mit/reading/reading.bib}

\title{Latent variable models}
\author{Abhishek Sarkar}
\date{}

\DeclareMathOperator\E{E}
\DeclareMathOperator\N{\mathcal{N}}
\newcommand\mb{\mathbf{B}}
\newcommand\me{\mathbf{E}}
\newcommand\mi{\mathbf{I}}
\newcommand\mr{\mathbf{R}}
\newcommand\msigma{\boldsymbol{\Sigma}}
\newcommand\mw{\mathbf{W}}
\newcommand\mx{\mathbf{X}}
\newcommand\my{\mathbf{Y}}
\newcommand\vb{\mathbf{b}}
\newcommand\vmu{\boldsymbol{\mu}}
\newcommand\vphi{\boldsymbol{\phi}}
\newcommand\vtheta{\boldsymbol{\theta}}
\newcommand\vx{\mathbf{x}}
\newcommand\vz{\mathbf{z}}

\begin{document}
\maketitle

\section{Introduction}

The essential aspect of a \emph{latent variable model} (LVM) is that each
observation $\vx_i$ (which is $p$-dimensional, say), has an associated latent
variable $\vz_i$ (which is $k$-dimensional). Usually, one assumes $k$ is much
less than $p$, corresponding to an assumption that the differences in the
high-dimensional observations $\vx_i$ are (mostly) explained by differences in
the lower-dimensional latent variables $\vz_i$.

\section{A simple example: Probabilistic PCA}

One of the simplest examples of an LVM is \emph{probabilistic PCA} (PPCA)
\cite{10.1111/1467-9868.00196}
%
\begin{align}
  \vx_i \mid \vz_i, \mw, \sigma^2 &\sim \N(\mw \vz_i, \sigma^2 \mi)\\
  \vz_i &\sim \N(\boldsymbol{0}, \mi).
\end{align}
%
In this model, observations $\vx_i$ are generated by first applying the linear
transform $\mw$ to the low-dimensional latent variable $\vz_i$, and then adding
multivariate Gaussian noise with covariance $\sigma^2 \mi$. There are two
inference goals for PPCA:

\begin{enumerate}
\item Estimate $\mw, \sigma^2$ given observed data $\vx_1, \ldots, \vx_n$. This
  involves maximizing the likelihood, that is, solving the optimization problem
  %
  \begin{align}
    \hat\mw, \hat\sigma^2 &= \arg\max_{\mw, \sigma^2} \sum_{i=1}^n \ln p(\vx_i \mid \mw, \sigma^2)\\
    &= \arg\max_{\mw, \sigma^2} \sum_{i=1}^n \ln \left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \mw, \sigma^2)\, dp(\vz_i)\right),
    \label{eq:eb}
  \end{align}
  %
  where $\mathcal{Z}$ denotes the set of values $\vz_i$ can take.
  
\item Estimate $\vz_i$ given $\vx_i$. This involves computing the posterior
  mean $\E[\vz_i \mid \vx_i, \hat\mw, \hat\sigma^2]$, using the result
  $\hat\mw, \hat\sigma^2$ from the previous step.
\end{enumerate}

How does one estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$? In
the case of PPCA, the integrals in equation \eqref{eq:eb} have closed forms
(following from properties of the Gaussian distribution); thus,
%
\begin{equation}
  \hat\mw, \hat\sigma^2 = \arg\max_{\mw, \sigma^2} \sum_{i=1}^n -\frac{1}{2}\ln\det(\mw\mw' + \sigma^2 \mi) - \frac{1}{2} \vx_i' (\mw\mw' + \sigma^2 \mi)^{-1} \vx_i.
  \label{eq:ppca}
\end{equation}
%
One can further show that the solution to the optimization problem
\eqref{eq:ppca} is unique and has closed forms related to the
eigendecomposition of $\mx'\mx$, where $\mx$ denotes the $n \times p$ matrix
$(\vx_1, \ldots, \vx_n)'$. Specifically, $\hat\mw$ is the matrix of the first
$k$ eigenvectors (having largest eigenvalues), and $\hat\sigma^2$ is the ratio
of the sum of the remaining $p - k$ eigenvalues to the sum of all the
eigenvalues. (Equivalently, the closed forms are related to the singular value
decomposition of $\mx$.)

How does one estimate $\vz_i$ given $\vx_i$? One can think of $p(\vx_i \mid
\vz_i, \mw, \sigma^2)$ as a likelihood and $p(\vz_i)$ as the prior in a
Bayesian setting. Both are Gaussian, and therefore conjugate; therefore, the
posterior is also Gaussian:
%
\begin{equation}
  \vz_i \mid \vx_i, \mw, \sigma^2 \sim \N((\mw'\mw + \sigma^2 \mi)^{-1} \mw' \vx_i, \sigma^2 (\mw'\mw + \sigma^2 \mi)^{-1}),
\end{equation}
%
yielding the closed-form posterior mean $(\hat\mw'\hat\mw + \hat\sigma^2
\mi)^{-1} \hat\mw' \vx_i$.

\section{The general scheme}

An LVM is typically given as a joint distribution
%
\begin{equation}
  p(\vx_i, \vz_i \mid \vtheta) = p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta),
  \label{eq:joint}
\end{equation}
%
where $\vtheta$ denotes a collection of additional model parameters. One can
think of $p(\vx_i \mid \vz_i, \vtheta)$ as the likelihood, and $p(\vz_i \mid
\vtheta)$ as the prior in a Bayesian setting. There are two inference goals for
LVMs:

\begin{enumerate}
\item Estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$. This
  involves solving an optimization problem
  %
  \begin{equation}
    \hat\vtheta = \arg\max_{\vtheta} \sum_{i=1}^n \ln \left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta)\, dp(\vz_i \mid \vtheta)\right),
  \end{equation}
  %
  where $\mathcal{Z}$ denotes the set of values $\vz_i$ can take. This problem
  may not be tractable, and may require approximate inference methods. (In the
  case of PPCA, the integrals had closed forms.) In a Bayesian setting, this
  task corresponds to estimating the prior from the data, which is termed
  \emph{empirical Bayes}. (In the case of PPCA, the prior did not have any free
  parameters to be estimated; however, in other models, the prior does have
  such parameters.)
  
\item Estimate $\vz_i$ given $\vx_i, \hat\vtheta$. This involves computing the
  posterior mean, by combining the likelihood with the prior estimated in the
  previous step. As above, this may not be tractable, and may require
  approximate inference methods. (In the case of PPCA, the prior was conjugate
  to the likelihood; therefore, the posterior mean had a closed form.)
\end{enumerate}
%
One may also want to generate new observations $\vx$. However, this is not
``inference'' as commonly understood in statistics; it is merely a case of
random sampling from the distribution $p(\vx \mid \hat\vtheta)$, which is
achieved by first sampling $\vz \sim p(\vz \mid \hat\vtheta)$, then sampling $\vx
\sim p(\vx \mid \vz, \hat\vtheta)$.

\section{Variational autoencoder}

A more complex LVM is the \emph{variational autoencoder} (VAE)
\cite{DBLP:journals/corr/KingmaW13}
%
\begin{align}
  \vx_i \mid \vz_i, \vtheta &\sim \N(f(\vz_i), \sigma^2 \mi)\\
  \vz_i &\sim \N(\boldsymbol{0}, \mi),
\end{align}
%
where $f$ denotes a neural network with $k$-dimensional input and
$p$-dimensional output, and $\vtheta$ denotes $\sigma^2$ and the parameters of
$f$. As a simple example, suppose $f$ is a fully-connected feed-forward network
with one hidden layer and a linear output layer, which can be written
%
\begin{equation}
  f(\vz_i) = \mw_1 h(\mw_0 \vz_i + \vb_0) + \vb_1,
\end{equation}
%
where $h$ is a non-linearity (say, ReLU) applied element-wise. In this case,
$\vtheta = (\sigma^2, \mw_0, \mw_1, \vb_0, \vb_1)$, and the VAE is a non-linear
version of PPCA. Specifically, one generates observations $\vx_i$ by applying
the non-linear transform $f$ to the low-dimensional latent variable $\vz_i$,
and then adding multivariate Gaussian noise with covariance $\sigma^2 \mi$. As
was the case for PPCA, the transform $f$ will be learned from the data (in
PPCA, the transform was $\mw$); however, unlike PPCA, in general it does not
have a closed form.

How does one estimate $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$? The
integrals in equation \eqref{eq:eb} do not have closed forms for the VAE, so one needs
to take a different strategy. The strategy taken for VAEs is \emph{variational
inference} \cite{10.1080/01621459.2017.1285773}. Recall the objective function,
%
\begin{align}
  &\quad \max_{\vtheta} \sum_{i=1}^n \ln\left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta) \,d\vz_i\right)\\
  \intertext{Introducing a distribution $q(\vz_i \mid \vphi)$ (sometimes termed
    the \emph{variational surrogate} or \emph{variational approximation}) by
    multiplying and dividing by its density, and optimizing over its parameters
    $\vphi$ also,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln\left(\int_{\mathcal{Z}} p(\vx_i \mid \vz_i, \vtheta) \frac{q(\vz_i \mid \vphi)}{q(\vz_i \mid \vphi)} p(\vz_i \mid \vtheta)\,d\vz_i\right) \label{eq:q}\\
  \intertext{By the definition of expectation with respect to $q(\vz_i \mid \vphi)$,}
  &= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln\left(\E_q\left[\frac{p(\vx_i \mid \vz_i, \vtheta) p(\vz_i \mid \vtheta)}{q(\vz_i \mid \vphi)}\right]\right)\\
  \intertext{Finally, one can exchange log and expectation using Jensen's
    inequality,}
  &\geq \max_{\vphi, \vtheta} \sum_i \E_q[\ln p(\vx_i \mid \vz_i, \vtheta) + \ln p(\vz_i \mid \vtheta) - \ln q(\vz_i \mid \vphi)] \label{eq:elbo}.
\end{align}

Equation \eqref{eq:elbo} is the objective function for variational inference in
LVMs, which can be optimized by a number of different algorithms, for example,
gradient descent. (Variational inference in other models involves an objective
function with similar form, involving the expectation of the joint probability
density $\E_q[\ln p]$ and the expectation of the variational density $\E_q[\ln
  q]$.)

Before detailing how this objective function is optimized for VAEs
specifically, first note that equation \eqref{eq:q} holds for any distribution
$q$ (since we multiplied and divided by its density); however, one can show
that the optimal $q$ (equivalently, optimal $\vphi$) is the true posterior
$p(\vz_i \mid \vx_i, \vtheta)$ (equivalently, the parameters of this
distribution) \cite{Neal1998}. Adding and subtracting $\ln p(\vx_i \mid
\vtheta)$,
%
\begin{align}
  \begin{split}
    &= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta) + \ln
      p(\vx_i \mid \vz_i, \vtheta) + \ln p(\vz_i \mid \vtheta)\\
      &\qquad\qquad\qquad - \ln p(\vx_i \mid \vtheta) - \ln q(\vz_i \mid \vphi)]\\
  \end{split}
\intertext{Using the definition of conditional probability,}
&= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta) - \ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)]\\
\intertext{Using linearity of expectation,}
&= \max_{\vphi, \vtheta} \sum_{i=1}^n \E_q[\ln p(\vx_i \mid \vtheta)] - \E_q[\ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)]
\intertext{Since $p(\vx_i \mid \vtheta)$ does not depend on $\vz_i$,
  $\E_q[\ln p(\vx_i \mid \vtheta)] = \ln p(\vx_i \mid \vtheta)$, and}
&= \max_{\vphi, \vtheta} \sum_{i=1}^n \ln p(\vx_i \mid \vtheta) - \E_q[\ln p(\vz_i \mid \vx_i, \vtheta) - \ln q(\vz_i \mid \vphi)] \label {eq:kl}
\intertext{The latter term is the definition of the KL divergence between
  $q(\vz_i \mid \vphi)$ and $p(\vz_i \mid \vx_i, \vtheta)$; it equals zero when
  the two distributions are the
  same. Thus, the optimal $q$ (equivalently, optimal $\phi$), holding $\vtheta$
  fixed, is the true posterior given $\vtheta$ (equivalently, its parameters),
  and the inequality \eqref{eq:elbo} becomes an equality}
&= \max_{\vtheta} \sum_{i=1}^n \ln p(\vx_i \mid \vtheta).
\end{align}

%% If one alternately computes $q(\vz_i \mid \vtheta) = p(\vz_i \mid \vx_i,
%% \vtheta)$ (that is, optimizes \eqref{eq:elbo} with respect to $q$, where $q$
%% can be any distribution and $\vtheta$ is fixed), and then optimizes
%% \eqref{eq:elbo} with respect to $\vtheta$ (holding $q$ fixed), one recovers the
%% EM algorithm.

However, for many models (including VAEs), the true posterior does not have a
closed form. In this case, optimizing the objective function \eqref{eq:elbo}
with respect to $q$, where $q$ can be any distribution, is difficult (since one
needs to find parameters $\vphi$ that can represent any distribution). So,
instead one optimizes over some restricted family of distributions $q \in
\mathcal{Q}$ (equivalently, some specific choice of parameter space for the
values $\vphi$), which may not contain the true posterior.

For example, one may assume that $q$ is a Gaussian distribution with unknown
mean $\vmu$ and unknown covariance matrix $\msigma$ (since then the posterior
mean is easy to compute; not since this is necessarily the true posterior), so
$\vphi = (\vmu, \msigma)$. If the true posterior is not Gaussian, the
inequality \eqref{eq:elbo} is strict, and is termed the \emph{evidence lower
bound} (ELBO). And, from equation \eqref{eq:kl}, maximizing the ELBO with
respect to $\vphi$ holding $\vtheta$ fixed is equivalent to minimizing the KL
divergence between $q(\vz_i \mid \vphi)$ and $p(\vz_i \mid \vx_i, \vtheta)$
holding $\vtheta$ fixed. Thus, the result will be the ``best'' approximating
distribution $q$, in the sense that it has minimum KL divergence from the true
posterior (given $\vtheta$), despite not being able to compute the true
posterior! Further, because one assumed $q$ was Gaussian, computing the
approximate posterior mean is trivial -- it is just $\vmu$.

In a VAE, the model is parameterized by a neural network $f$, termed the
\emph{decoder network} (since it ``decodes'' the low-dimensional latent
variable $\vz_i$ to produce the observed $\vx_i$, minus the noise). Therefore,
it is natural to choose $q$ to also be parameterized by a neural network,
termed the \emph{encoder network} (since it ``encodes'' $\vx_i$ to produce
$\vz_i$). Specifically,
%
\begin{equation}
  q(\vz_i \mid \vx_i, \vphi) = \N(m(\vx_i), \operatorname{diag}(s^2(\vx_i))),
\end{equation}
%
where $m, s^2$ are each $k$-dimensional outputs of a neural network taking
$p$-dimensional input, denoting the mean and diagonal of the covariance matrix,
respectively. In essence, one assumes the posterior is multivariate Gaussian
with diagonal covariance, which is a special case of the example we gave above.
(The mean and covariance matrix depend on the data $\vx_i$ for computational
reasons \cite{Gershman2014}).

Thus, one estimates $\vtheta$ given observed data $\vx_1, \ldots, \vx_n$ by
optimizing the ELBO \eqref{eq:elbo} with respect to $\vphi, \vtheta$ (i.e.,
$\sigma^2$ and the neural network parameters of $f, m, s^2$). This procedure
also yields an estimate $\hat\vphi$, corresponding to the approximate posterior
distribution $q$ (equivalently, the neural network parameters of $m, s^2$). For
VAEs, this is achieved by stochastic optimization via gradient descent
\cite{DBLP:journals/corr/KingmaW13}. Briefly, the fundamental difficulty is
that \eqref{eq:elbo} does not have a closed form, and so it is replaced by a
stochastic objective function that does have a closed form, whose expected
value is the true objective function value, and whose gradient can be computed
using backpropagation.

How does one estimate $\vz_i$ given $\vx_i$? The approximate posterior mean is
$m(\vx_i)$, that is, the output of the encoder network.

\printbibliography

\end{document}
